{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03b64b5-8b96-4ae2-a672-39afd2133b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b021264-1dd7-4374-a4d1-34dfdd8fe02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "press_list = ['경향신문', '국민일보', '뉴스1', '뉴시스', '동아일보', '문화일보', '서울신문', '세계일보', '연합뉴스',  '조선일보', '중앙일보', '한겨레', '한국일보']\n",
    "\n",
    "stop_content = [\"무단전재\", \"재배포금지\",\"저작권자 ⓒ 서울신문사\",\"무단복제 및 전재\",\"무단 전재 및 재배포\",\"제보는 카톡\", \"☞\", \"무단 전재-재배포\", \"▶연합뉴스 앱 지금 바로 다운받기~\"]\n",
    "\n",
    "# rep_list = [u'\\xa0', u'\\u2027', u'\\u30fb', u'\\u2024', u'\\uff65', u'\\u2014', u'\\u22c5', u'\\u207a', u'\\u2219', u'\\u2e31', u'\\u200d']\n",
    "rep_list = ['기사내용 요약']\n",
    "\n",
    "stop_title = ['[부고]', '[인사]', '[표]', '[모멘트]', '[궂긴소식]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e29f31-919e-48c8-a6b8-b17a8068561d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 함수 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5522c08f-24b9-4232-b4dc-8e2bd9021db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(url):\n",
    "    i = 0\n",
    "    ## 요청 오류시 10번  재시도\n",
    "    while i < 10 :\n",
    "        try:\n",
    "            res = requests.get(url)\n",
    "            res.raise_for_status()\n",
    "            soup = BeautifulSoup(res.content, 'html.parser', from_encoding='cp949')\n",
    "            break\n",
    "        except:\n",
    "            i += 1\n",
    "            \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769ae014-03a1-4df9-bad6-39a032e66b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_scraper(link):\n",
    "    article_soup = create_soup(link)\n",
    "    \n",
    "    # 기자없는 기사 걸러내기\n",
    "    journalist = article_soup.select('div>span')[0].text.split('기자')[0].strip() + ' 기자'\n",
    "    if '입력' in journalist or journalist == ' 기자':\n",
    "        return False\n",
    "    \n",
    "    # 본문 정리\n",
    "    article = article_soup.find_all('section')[1]\n",
    "    content = article.find_all(True, attrs={'dmcf-ptype':'general'})\n",
    "    rst = []\n",
    "    for para in content:\n",
    "        for tmp in para.text.split('\\n'):\n",
    "            if tmp.strip() != '':\n",
    "                rst.append(tmp.strip())\n",
    "    content = []\n",
    "    for c in rst:\n",
    "        for i in stop_content:\n",
    "            if i in c:\n",
    "                break\n",
    "        else:\n",
    "            for rep in rep_list:\n",
    "                c = c.replace(rep, '')\n",
    "            content.append(c)\n",
    "    content = ' '.join(content)\n",
    "    if len(content) < 200:\n",
    "        return False\n",
    "    return content, journalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c497673-f3a6-4f61-8509-dfd71dc38b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(date, category):\n",
    "    articles = []\n",
    "    # 마지막 리스트 페이지 체크\n",
    "    url = f'https://news.daum.net/breakingnews/{category}?page=999&regDate={date}'\n",
    "    soup = create_soup(url)\n",
    "    last_page = soup.find(\"em\",attrs ={\"class\": \"num_page\"})\n",
    "    last_page_num = int(re.sub(r'[^0-9]', '', last_page.text))\n",
    "    # last_page_num = 10\n",
    "    \n",
    "    for page in tqdm(range(1, last_page_num+1), desc=date):\n",
    "        url = f'https://news.daum.net/breakingnews/{category}?page={page}&regDate={date}'\n",
    "        soup = create_soup(url)\n",
    "        news_list = soup.find_all('strong', attrs={'class':'tit_thumb'})\n",
    "        \n",
    "        # 리스트의 뉴스 하나씩 처리\n",
    "        for i in range(len(news_list)-4):\n",
    "            article = []\n",
    "            press = news_list[i].find('span').text.split('·')[0].strip()\n",
    "            if press not in press_list:\n",
    "                continue\n",
    "            title = news_list[i].find('a').text\n",
    "            tflag = True\n",
    "            for t in stop_title:\n",
    "                if t in title:\n",
    "                    tflag = False\n",
    "                    break\n",
    "            if not tflag:\n",
    "                continue\n",
    "            link = news_list[i].find('a')['href']\n",
    "            # print(link, end='\\r')\n",
    "            date = news_list[i].find('a')['href'].split('/')[-1][:8]\n",
    "            arcid = news_list[i].find('a')['href'].split('/')[-1][8:]\n",
    "            category = url.split('/')[-1].split('?')[0]\n",
    "            checker = content_scraper(link)\n",
    "            if not checker:\n",
    "                continue\n",
    "            content, journalist = checker\n",
    "            \n",
    "            article = [arcid, press, title, content, journalist, date, link, category]\n",
    "            articles.append(article)\n",
    "    df = pd.DataFrame(articles, columns=['arcid', 'press', 'title', 'content', 'journalist', 'date', 'link', 'category'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b480be-ed04-4346-9017-598a385a4055",
   "metadata": {},
   "source": [
    "# 기간 설정 스크래핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de1726c2-10bb-4e73-9a4b-e5a16854dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from dateutil.rrule import rrule, DAILY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d999da0e-567b-4f6b-9a63-ff7a2289ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2021, 12, 6)\n",
    "end_date = date(2021, 12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbf06e4c-4e8a-40cf-a4c4-eea647fc029f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20211206: 100%|██████████████████████████████████████████████████████████████████████| 471/471 [08:17<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "log = []\n",
    "for date_dt in list(rrule(DAILY, dtstart=start_date, until=end_date)):\n",
    "    try:\n",
    "        df_day = pd.DataFrame()\n",
    "        date_s = date_dt.strftime('%Y%m%d')\n",
    "        df_day = pd.concat([df_day, get_articles(date_s, 'society')])\n",
    "        df_day.reset_index(drop=True, inplace=True)\n",
    "        df_day.to_csv(f'../data/{date_s}_society.csv', index=False)\n",
    "    except:\n",
    "        ## 최종적으로 실패 시 로그에 해당 날짜 추가\n",
    "        log.append(date_s)\n",
    "        print(date_s)\n",
    "\n",
    "if len(log) > 0 :\n",
    "    logdf = pd.DataFrame(log)\n",
    "    logdf.to_csv(f'../data/faillog_society.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d323b402-fbb3-4c3c-b8e9-c797c16780e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
