{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "317b9a1a-0535-4469-bf71-f2c18aad90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "news_path_list = glob.glob('testdata/*.json', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32b65e47-24de-4751-a9f6-20e908332ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# news_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a50648b6-e5c2-4d89-8c70-f9bb03329787",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx ,new_path in enumerate(news_path_list):\n",
    "    news_path_list[idx] = new_path.replace(\"\\\\\", \"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab993b21-2dab-43fd-b1f6-b8f924eb8de7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# news_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca994fab-c491-4b7e-b972-65a48406a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c31298e6-5cce-49a9-94dd-6173beaa4478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = news_path_list[0]\n",
    "# with open(file_path, 'r',encoding =\"utf-8\" ) as file:\n",
    "#     data = json.load(file)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b67f1e74-77dc-4d2a-99a9-4a412318b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Meta(Acqusition)\"][\"doc_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df80427d-820f-489d-9733-2d75f013d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Meta(Acqusition)\"][\"publisher_year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3586931-b8a8-479e-b86a-e14a3b79c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Meta(Refine)\"][\"passage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91bd89cc-65d9-41ca-aed4-21e9f1a7247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title = []\n",
    "news_year = []\n",
    "news_content = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1da0ef5e-ad31-4a45-b55b-b01c4461be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for news_path in news_path_list:\n",
    "    file_path = news_path\n",
    "    with open(file_path, 'r',encoding =\"utf-8\" ) as file:\n",
    "        data = json.load(file)\n",
    "    news_title.append(data[\"Meta(Acqusition)\"][\"doc_name\"])\n",
    "    news_year.append(data[\"Meta(Acqusition)\"][\"publisher_year\"])\n",
    "    news_content.append(data[\"Meta(Refine)\"][\"passage\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "401cfefa-0844-4b3b-b9b5-6f326801bfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title_ = []\n",
    "news_year_ = []\n",
    "news_content_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a71634c-a696-438a-af3d-f9db973400f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_title = \" \"\n",
    "tmp_idx = 0\n",
    "for idx, title in enumerate(news_title):\n",
    "    if tmp_title == title:\n",
    "        news_content_[tmp_idx-1] = news_content_[tmp_idx-1] + news_content[idx]\n",
    "    else :\n",
    "        tmp_title = title\n",
    "        tmp_idx += 1\n",
    "        news_title_.append(news_title[idx])\n",
    "        news_year_.append(news_year[idx])\n",
    "        news_content_.append(news_content[idx])\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cecdc86a-51b0-44a6-83f9-c65c8fc0e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, content in enumerate(news_content_):\n",
    "    news_content_[idx] = content.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1dbec42f-6e63-49d8-a6b4-ee83277a35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c286c25-5b9c-4c79-9828-af280a36b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(zip(news_title_,news_year_, news_content_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e84c4e51-c465-45b8-97b4-c85287d5174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"title\" , \"year\", \"content_ori\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e569612-3325-48eb-acac-ecc2bf64073a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd558fb2-188c-42ce-905d-886f506a7ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>content_ori</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40억달러 딜 주인공 김봉진 \"국내서 폼잡다 죽고싶지 않았다\"</td>\n",
       "      <td>2020</td>\n",
       "      <td>40억 달러 ‘딜’ 주인공 김봉진 우아한형제들 대표태풍 뒤의 고요함이랄까.   40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[차이나인사이트] 한한령 풀리면 한·중관계 회복될까</td>\n",
       "      <td>2020</td>\n",
       "      <td>시진핑 방한과 새해 한·중관계 전망지난달 23일 베이징에서 열린 한·중 정상회담을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[최상연 논설위원이 간다] 이재오 “무반성 한국당, 무능 지도부…이대론 총선 어렵다”</td>\n",
       "      <td>2020</td>\n",
       "      <td>보수진영 사분오열 속 ‘국민통합연대’ 띄운 비박계 크리스마스를 앞둔 지난 23일 오...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[이철호 칼럼] 문 대통령, 총알을 물어야 한다</td>\n",
       "      <td>2020</td>\n",
       "      <td>배달의민족이 독일 자본에 매각된 것을 놓고 말들이 많다.   민족 정서를 배반했다며...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'北핵무력 강화 핵심' 이병철, 노동당 정치국 위원에 선출</td>\n",
       "      <td>2020</td>\n",
       "      <td>지난 28일부터 나흘간 진행된 7기 5차 노동당 전원회의에서 북한은 핵 무력 개발의...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18817</th>\n",
       "      <td>[더오래]치매 아버지 뜻 무시하고 정신병원 입원시킨 자식</td>\n",
       "      <td>2020</td>\n",
       "      <td>[더,오래] 이형종의 초고령사회 일본에서 배운다(64)“왜 주위 사람들이 내가 바라...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18818</th>\n",
       "      <td>사라진 12살 아들…태항산 4계절 누빈 경찰 엄마 '20년 추적'</td>\n",
       "      <td>2020</td>\n",
       "      <td>아들을 찾아 20년 세월 동안 중국 곳곳을 누비고 다니는 한 엄마의 사연이 초겨울 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18819</th>\n",
       "      <td>기후환경회의 \"2035년 국내 신차는 친환경차만, 2040년 석탄발전 중단\"</td>\n",
       "      <td>2020</td>\n",
       "      <td>'2040년 탈석탄, 2035년 탈내연기관' 국내 각 분야 전문가들이 모여 기후변...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18820</th>\n",
       "      <td>서울시 통화연결음이 '새 광화문광장'…野 \"성형집착 이유 뭐냐\"</td>\n",
       "      <td>2020</td>\n",
       "      <td>또 “국가적 위기 속에 시민 혈세를 낭비하는 2021년도 광화문광장 재구조화 예산...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18821</th>\n",
       "      <td>“회식하고 확진되면 징계?”…특별방역 지침에 공직사회 술렁</td>\n",
       "      <td>2020</td>\n",
       "      <td>“모임을 했다가 확진되면 문책을 한다는데, 말이 되나요?” 23일 서울시의 한 공무...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18822 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  year  \\\n",
       "0                  40억달러 딜 주인공 김봉진 \"국내서 폼잡다 죽고싶지 않았다\"   2020   \n",
       "1                         [차이나인사이트] 한한령 풀리면 한·중관계 회복될까  2020   \n",
       "2      [최상연 논설위원이 간다] 이재오 “무반성 한국당, 무능 지도부…이대론 총선 어렵다”  2020   \n",
       "3                           [이철호 칼럼] 문 대통령, 총알을 물어야 한다  2020   \n",
       "4                     '北핵무력 강화 핵심' 이병철, 노동당 정치국 위원에 선출  2020   \n",
       "...                                                ...   ...   \n",
       "18817                  [더오래]치매 아버지 뜻 무시하고 정신병원 입원시킨 자식  2020   \n",
       "18818             사라진 12살 아들…태항산 4계절 누빈 경찰 엄마 '20년 추적'  2020   \n",
       "18819       기후환경회의 \"2035년 국내 신차는 친환경차만, 2040년 석탄발전 중단\"  2020   \n",
       "18820              서울시 통화연결음이 '새 광화문광장'…野 \"성형집착 이유 뭐냐\"  2020   \n",
       "18821                 “회식하고 확진되면 징계?”…특별방역 지침에 공직사회 술렁  2020   \n",
       "\n",
       "                                             content_ori  \n",
       "0      40억 달러 ‘딜’ 주인공 김봉진 우아한형제들 대표태풍 뒤의 고요함이랄까.   40...  \n",
       "1      시진핑 방한과 새해 한·중관계 전망지난달 23일 베이징에서 열린 한·중 정상회담을 ...  \n",
       "2      보수진영 사분오열 속 ‘국민통합연대’ 띄운 비박계 크리스마스를 앞둔 지난 23일 오...  \n",
       "3      배달의민족이 독일 자본에 매각된 것을 놓고 말들이 많다.   민족 정서를 배반했다며...  \n",
       "4      지난 28일부터 나흘간 진행된 7기 5차 노동당 전원회의에서 북한은 핵 무력 개발의...  \n",
       "...                                                  ...  \n",
       "18817  [더,오래] 이형종의 초고령사회 일본에서 배운다(64)“왜 주위 사람들이 내가 바라...  \n",
       "18818  아들을 찾아 20년 세월 동안 중국 곳곳을 누비고 다니는 한 엄마의 사연이 초겨울 ...  \n",
       "18819   '2040년 탈석탄, 2035년 탈내연기관' 국내 각 분야 전문가들이 모여 기후변...  \n",
       "18820   또 “국가적 위기 속에 시민 혈세를 낭비하는 2021년도 광화문광장 재구조화 예산...  \n",
       "18821  “모임을 했다가 확진되면 문책을 한다는데, 말이 되나요?” 23일 서울시의 한 공무...  \n",
       "\n",
       "[18822 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0248d8c-d5e5-40d5-8e51-da8fea7cfb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\park\\AppData\\Local\\Temp\\ipykernel_6620\\548626087.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['content'] = df['content_ori'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n"
     ]
    }
   ],
   "source": [
    "# 한글 이외 제거\n",
    "df['content'] = df['content_ori'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35ac53a7-5409-46bb-9ebf-70077a2c6781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        억 달러 딜 주인공 김봉진 우아한형제들 대표태풍 뒤의 고요함이랄까   억 달러짜리 ...\n",
       "1        시진핑 방한과 새해 한중관계 전망지난달 일 베이징에서 열린 한중 정상회담을 계기로 ...\n",
       "2        보수진영 사분오열 속 국민통합연대 띄운 비박계 크리스마스를 앞둔 지난 일 오전 서울...\n",
       "3        배달의민족이 독일 자본에 매각된 것을 놓고 말들이 많다   민족 정서를 배반했다며 ...\n",
       "4        지난 일부터 나흘간 진행된 기 차 노동당 전원회의에서 북한은 핵 무력 개발의 주역인...\n",
       "                               ...                        \n",
       "18817    더오래 이형종의 초고령사회 일본에서 배운다왜 주위 사람들이 내가 바라지도 않는 일을...\n",
       "18818    아들을 찾아 년 세월 동안 중국 곳곳을 누비고 다니는 한 엄마의 사연이 초겨울 문턱...\n",
       "18819     년 탈석탄 년 탈내연기관 국내 각 분야 전문가들이 모여 기후변화미세먼지 대응을 위...\n",
       "18820     또 국가적 위기 속에 시민 혈세를 낭비하는 년도 광화문광장 재구조화 예산을 자진 ...\n",
       "18821    모임을 했다가 확진되면 문책을 한다는데 말이 되나요 일 서울시의 한 공무원은 정부가...\n",
       "Name: content, Length: 18822, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a87c09e0-73a3-46af-b71d-7294cb6c2ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "\n",
    "stopwords = [\"의\",\"가\",\"이\",\"은\",\"는\",\n",
    "             \"들\",\"좀\",\"잘\",\"강\",\"과\",\n",
    "             \"도\",\"를\",\"으로\",\"자\",\"에\",\n",
    "             \"와\",\"한\",\"하다\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d59f850-d6ff-4e39-8c39-90407ff4c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer(tokenizer = okt.nouns, stop_words= stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0770d78-5f72-43a0-b608-f3e3afb1808c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c0a32-c5b5-49e7-9877-bbce10faddf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "def14ebb-d3d6-41cf-9658-d3a053ae529f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebc470ff2ef4dcfbc547b5aa55da033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18822 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tfidf_dtm = tfidf_vec.fit_transform(notebook.tqdm(df[\"content\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e04f58f0-021b-4000-aa9a-ead23cbc8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okt 관련해서 피클이 오류남\n",
    "# import pickle\n",
    "# with open('news_tfidf_vec.p', 'wb') as file:\n",
    "#     pickle.dump(tfidf_vec, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc70a7c-4062-47b7-9cb0-70298d2af8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d5b61d0-9225-47d4-966f-724c7102028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content = \"\"\"이창양 산업통상자원부 장관이 29일 도시가스요금 할인을 적용받고 있는 사회복지시설을 찾아 취약계층 보호를 위한 지원을 확대할 계획이라고 밝혔다.\n",
    "\n",
    "이 장관은 이날 서울 성북구 정릉노인요양원을 방문해 “우크라이나 전쟁 등으로 급등한 천연가스 가격으로 인해 에너지 취약계층 등을 비롯한 일반 국민들께서 금번 동절기 높은 난방비로 어려움을 겪고 있는 것에 대해 에너지 주무장관으로서 마음이 무겁다”고 말했다.\n",
    "\n",
    "산업부는 사회복지시설이 평균적으로 42% 요금 할인 혜택을 받는다고 설명했다. 기존 산업용 요금 대신 가장 저렴한 민수용 요금을 적용했기 때문이다. 산업부는 “정릉 노인요양원의 경우에는 12월 사용량에 대한 가스요금이 당초 314만원에서 45.5% 할인(143만원)된 171만원이 적용된다”고 했다.\n",
    "\n",
    "앞서 산업부는 지난 18일 ‘사회복지시설 등에 대한 도시가스요금 경감지침’을 개정해 2023년 1월1일부터 3월31일까지 사용한 도시가스에 대해 일반용 요금을 적용하겠다고 밝힌 바 있다. 이와 관련 산업부 관계자는 “지난해 12월 도시가스 요금부터 적용하는 것으로 지침을 다시 개정할 것”이라고 설명했다.\n",
    "\n",
    "또한 산업부는 취약계층에 대한 가스요금 할인액을 지난해 대비 3배 인상(6천원∼2만4천원→1만8천원∼7만2천원)하고, 동절기 에너지 바우처 지원금액을 지난해 동절기 대비 약 2.6배 인상(11만8천원→30만4천원)했다고 밝혔다.\n",
    "\n",
    "이 장관은 “정부는 겨울철 난방 수요가 집중되는 점을 감안해 올해 1분기는 가스요금을 동결했다”며 “향후에도 불가피하게 가스요금이 인상될 경우 관계부처와 협의하여 추가적인 지원대책을 마련하는 등 에너지 취약계층을 두텁게 보호할 계획”이라고 밝혔다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e780c8b-2a31-4697-ad20-bfce4c117adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content = test_content.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "tfidf_test = tfidf_vec.transform([test_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "777466f1-65b0-4609-ba3b-590236393b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim_res = cosine_similarity(tfidf_test,tfidf_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eefb1fca-3c2b-4cfd-ab77-719c5ed0de25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2644, 18427, 15072, 8478, 9779]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_idx = [top_list[0] for top_list in sim_scores_n]\n",
    "top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f019b022-3a27-4fc2-8500-14e8b5ee98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation(content, n):\n",
    "    sim_scores = list(enumerate(cos_sim_res[0]))\n",
    "    sim_scores = sorted(sim_scores, key = lambda x : x[1], reverse=True)\n",
    "    sim_scores_n = sim_scores[0:n]\n",
    "    top_idx = [top_list[0] for top_list in sim_scores_n]\n",
    "    return df['title'].iloc[top_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d54109f9-8374-4b86-8eb7-5acbad2027b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_to_recom (content, n):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    test_content = content.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "    tfidf_test = tfidf_vec.transform([test_content])\n",
    "    cos_sim_res = cosine_similarity(tfidf_test,tfidf_dtm)\n",
    "\n",
    "    sim_scores = list(enumerate(cos_sim_res[0]))\n",
    "    sim_scores = sorted(sim_scores, key = lambda x : x[1], reverse=True)\n",
    "    sim_scores_n = sim_scores[0:n]\n",
    "    top_idx = [top_list[0] for top_list in sim_scores_n]\n",
    "    return df['title'].iloc[top_idx]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e836060f-b384-4123-bf73-ae92c6674545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644             서울 가스요금 차등화…강남·은평 오르고 구로·중구 내릴 듯\n",
       "18427    지하철·버스 1.5조 적자…전문가 \"요금 인상\" 시민단체 \"자구노력부터\"\n",
       "15072              소상공인ㆍ저소득층, 전기·가스요금 납부기한 3개월 연장\n",
       "8478        \"한국 통신비 비싸다?\"…30년 만의 요금 인가제 폐지로 다시 논란\n",
       "9779           1조8000억 깎아준 경차·출퇴근 통행료 할인, 이번엔 손본다\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_to_recom(test_content,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "584ab44d-ebce-4e39-9062-777ea56f7a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19년간 단일 가격 체제를 유지했던 서울시 가스요금 부과방식이 수술대에 올랐다.   서울시는 18일 도시가스 공급 업체별로 다른 가스요금을 책정하는 ‘개별요금제’ 도입을 검토하고 있다고 밝혔다.   서울시에 따르면 가스요금제 개편을 위한 용역작업은 현재 마무리 단계에 있다.   업계는 이르면 올해 7월부터 바뀐 요금제가 적용될 것으로 내다보고 있다.   1993년 전국에 가스가 보급되며 가스요금을 정하는 권한은 각 시·도로 넘어왔다.   매년 7월 1일자로 가스요금을 결정하는 서울시는 그동안 ‘총평균 방식’을 고수해왔다.   서울 시내에 가스를 공급하는 5개 업체(서울도시가스, 코원에너지, 예스코, 귀뚜라미에너지, 대륜E&S)의 비용이 요금에 반영할수 있도록 평균을 내 가스요금을 정하는 것이다.   업체는 천연가스를 들여와 배관을 깔고 이를 공급해 거기서 수익을 낸다.   업체 입장에선 가스요금이 ‘평균’으로 맞춰지다보니 손익 발생 구조가 왜곡됐다.   서울시가 정한 가스요금보다 회사의 원가가 낮으면 돈을 벌고, 원가가 높으면 손해를 보는 구조다.   ‘총평균 방식’을 19년 이상 유지하다 보니 회사별 수익 격차가 커졌다.   더 큰 문제는 “원가를 낮추면 이익을 낼 수 있는 구조인 탓에 가스회사가 투자를 꺼려 서비스 질이 낮아진다”는 지적이 나온 것이다.   서울시가 가스요금제 개편 검토에 들어간 이유다.   새 가스 요금제의 핵심은 “5개 회사의 수익 차이를 줄여보자”는 데 있다.   이를 위해 고안한 것이 바로 ‘조정 계수’다.   5개사 비용 평균에 회사별로 다른 조정 계수를 적용해 가스요금을 다르게 만들어 수익 편차를 절반으로 낮추겠다는 것이다.   서울시 관계자는 “올해 처음으로 조정계수를 도입해보려 하고 있다”며 “구체적인 방안은 오는 6월에 나올 것”이라고 말했다.   예를 들어 한 달간 60㎥의 주택용 난방을 사용한 세대의 경우 가스요금은 4만7000원이다.   그런데 조정 계수를 도입하면 업체별 요금이 달라진다.   지난해 57억원대의 손실을 본 서울도시가스의 경우 기존 요금에 3.  2%를 더해 가스요금이 정해진다.   가입자는 100원을 더 내야 한다.   반면 지난해 45억원대 수익을 낸 귀뚜라미에너지의 경우 12%를 깎아 가스 요금은 376원 내려야 한다.   같은 가스를 공급받으면서도 비용은 제각각인 상황이 발생하게 된다.   현재 서초구와 종로구, 용산구, 성북구, 서대문구, 양천구의 경우 5개 업체 중 2개 회사가 같은 구에서 가스를 공급하고 있다.  '"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2644][\"content_ori\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28c8000c-7e77-48e1-a0e7-95436dd60832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1조5000억원대(2020년)에 이르는 서울 지하철·버스 적자 문제를 해결하려면 요금 인상이 불가피하다는 전문가 주장이 나왔다.   16일 서울 여의도 국회 의원회관에서 열린 ‘대중교통 재정 위기 극복을 위한 시민토론회’에서다.   하지만 시민단체는 이 토론회가 요금 인상을 위한 것이냐며 반발했다.    이날 토론회는 서울교통공사 등 전국 6개 도시철도 운영기관이 우원식·이해식·천준호(이하 더불어민주당)·이헌승(국민의힘)·이은주(정의당) 의원, 전국버스운송사업조합연합회와 공동 개최했다.    김형진 연세대 도시공학과 교수는 기조연설에서 “지하철·버스 등 대중교통의 만성 재정 적자가 노후시설 설비, 서비스 개선의 걸림돌이 되고 있다”며 “운영원가 절감, 수요 증대 노력도 필수지만 제도 개선을 통한 중앙정부의 재정 지원, 이용자의 협력이 필요한 때”라고 말했다.    김 교수는 1인당 소득 수준을 고려한 서울 도시철도 요금이 런던의 50%, 뉴욕의 70% 수준이라며 이용요금 현실화가 절실하다고 했다.    발제를 맡은 이신해 서울연구원 교통시스템연구실장은 “대중교통 요금이 싼 게 좋은 것인지 생각해봐야 한다”며 “팬데믹 사태에 따른 지방 도시 버스회사 적자로 노선 운영을 못 하게 되면 이를 재건하는 데 시간·비용이 들고 피해는 이용자에게 돌아간다”고 말했다.   요금 조정은 시민에게 부담을 부과하는 것이 아니라 대중교통 체계 유지를 위한 필요조건이라는 설명이다.   이 실장은 대중교통체계를 통합요금제로 개편한 2004년 이후 요금 조정주기가 길어져 예측가능성이 낮아졌다며 물가·인건비에 따라 정기적으로 요금을 인상·인하·평가하는 정례화가 필요하다고 제안했다.    구종원 서울시 교통기획관 역시 “국비, 재정 지원, 요금 인상, 자구책 4가지 방안의 불균형을 해소하기 위해 요금의 현실화가 불가피하다”며 “요금 조정을 정례화해 요금 인상의 정치적 색채를 지워야 한다”고 말했다.    서울교통공사에 따르면 현재까지 서울 지하철의 누적 적자는 15조원(1981년~2019년)에 이른다.   서울교통공사는 적자 요인으로 저렴한 운임, 법정 무임승차 등 공익서비스 증가, 시설 노후화, 코로나19에 따른 수요 급감을 꼽았다.    버스는 준공영제에 따라 서울시 재정 보전으로 유지되고 있는데 적자가 재정 보조가 가능한 규모를 넘어 버스조합 명의 대출로 충당하고 있다.   버스회사들의 자구 노력으로 750억원의 수익을 창출했지만 적자 규모의 11%에 불과하다.  '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[18427][\"content_ori\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836e935-1a47-4aed-a056-88cab39591ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79fbe272-5aab-469f-aa86-8f49ced25949",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content = \"\"\"제재를 받은 사업가 예브게니 프리고진이 설립한 러시아 용병 그룹 와그너는 수년 동안 주목을 받지 못했으며 엘리트 지배층은 이 용병단의 존재 자체를 부인했다.\n",
    "\n",
    "하지만 와그너 그룹은 러시아의 우크라이나 침공 과정에서 전장에서 점점 더 중요한 역할을 하기 시작했다. 전과자를 포함한 일부 전투원들은 러시아의 최고 훈장을 받았다.\n",
    "\n",
    "2022년 마지막 날, 블라디미르 푸틴 러시아 대통령은 우크라이나에서 싸운 장병들에게 국가 최고의 훈장을 수여하기 위해 남부를 방문했다. 그들 중 한 명은 수훈자 그룹의 나머지 사람들이 입는 것과는 다른 군복을 입은 수염 난 청년이었다. 그는 러시아 용병들이 입는 종류의 군복을 입었다.\n",
    "\n",
    "4만명 중 한 명\n",
    "그는 2019년 10월 모스크바의 한 카페에서 강도 미수 혐의로 체포되어 몇 달 후 징역 7년을 선고받은 무술 팬인 아이콤 가스파르얀으로 확인됐다. 그는 징역 7년을 선고 받은지 두 달만인 12월 바그너 그룹과 관련된 텔레그램 채널 중 하나에 올라온 영상에 등장해 자신이 랴잔의 감옥을 떠나 우크라이나에서 싸우고 있다고 말했다.\n",
    "\n",
    "가스파르얀은 미국이 우크라이나에 배치된 것으로 추정하는 4만 명의 러시아 전직 수감자 중 한 명이 되었다. 이들은 바그너 용병 그룹의 정규 계약자 1만 명과 함께 싸운다. 워싱턴 포스트에 따르면 이 수치는 수감자들의 전쟁 참여를 추적하고 있는 러시아 죄수 인권 단체 러시아 비하인드바스(Russia Behind Bars)가 수집한 데이터와 일치한다.\n",
    "그룹의 설립자인 예브게니 프리고진은 지난 여름 모병을 위해 러시아 교도소 시설을 방문했다. 그는 와그너 그룹에 가입하고 우크라이나에서 러시아의 전쟁에 참여한 죄수들에게 범죄 기록 말소를 약속했다. 나중에 이 사람들이 최전선에서 가장 위험한 곳으로 보내졌고 그 중 많은 사람들이 죽었다는 것이 밝혀졌다.\n",
    "\n",
    "우크라이나군은 바그너 그룹의 죄수들이 총알받이로 사용되는 것을 목격했고 대다수가 사망했다고 말했다.\n",
    "\n",
    "높은 보수, 그리고 모험 약속\n",
    "항상 수감자들에게 의존해 용병단을 꾸리지는 않았다.\n",
    "\n",
    "2014년에 설립되어 2015~2016년에 활동이 활발해진 이 용병 조직은 우크라이나 동부에서 러시아의 지원을 받는 분리주의자들을 돕기 위해 만들어졌다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bfb49a05-4855-4abc-be4c-1f44acef6ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17689    연세대 RC교육원 ‘한-러 수교 30주년 기념 러시아 문화의 달’ 행사\n",
       "5015          격리위치 추적, 어기면 5년 감옥 \"러시아, 소련때보다 지독\"\n",
       "5907          \"코로나 만든 건 美\" 음모론, 中보다 더 불지핀 건 이 나라\n",
       "507         우크라 \"추락 여객기, 이란 보유 러 미사일에 피격 가능성 검토\"\n",
       "10585       \"아프간 미군 살해 사주 알고도 놔뒀다\" 트럼프 또 '러 스캔들'\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_to_recom(test_content,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d2660542-5e44-432a-b70d-8d5744703ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'연세대학교 RC교육원과 노어노문학과는 주한러시아대사관과 러시아연방 국제인문학 협력청의 후원으로 10월 한 달간 ‘한-러 수교 30주년 기념 러시아 문화의 달’ 행사를 개최했다.   코로나19로 인해 비대면 수업이 진행되고 있는 상황에서 ‘러시아 문화의 달’ 프로그램은 유튜브 채널을 통해 다양한 영상을 제공하는 방식으로 진행됐다.   한국과 러시아는 1990년 9월 수교를 체결했다.   두 나라의 수교 체결은 냉전의 시대를 종결하고, 동북아에 굳건한 평화를 가져온 역사적 사건이다.   러시아는 동유럽에서 태평양에 걸쳐 세계 최대 영토를 보유한 국가로서 한국의 경제·통상 및 외교·안보에서 중요한 파트너이다.   최근 대한민국은 신북방정책을 통해 러시아를 포함한 유라시아 국가들과의 협력에 더욱 박차를 가하고 있다.   연세대 노어노문학과 학생들은 지난 7월부터 온라인 ‘러시아 문화의 달’ 행사를 위해 러시아를 소개할 수 있는 다양한 주제를 정해, 한국 대학생들이 러시아를 쉽게 이해할 수 있도록 하는 ’장벽 낮추기 프로젝트‘를 진행했다.   학생들이 기획한 ‘러시아의 모든 것’ 포스터 영상에서는 러시아 주요 도시 소개, 자연환경, 명절과 음식, 문학, 언어 등 쉽고 재미있는 러시아 이야기를 확인할 수 있다.   그밖에도 유튜브에서 제공되는 우수한 러시아 관련 콘텐츠를 선별해 소개의 글과 함께 링크를 제공하고 있으며, 연세대 RC학생들을 위한 온라인 퀴즈 이벤트도 진행됐다.   또한, 러시아 관광공사에서 제공하는 러시아 홍보 영상 및 러시아 자연, 환경 관련 다큐 영상물 등도 볼 수 있으며, 러시아 문화 특강 시리즈를 통해 러시아 문화, 종교, 역사에 대한 흥미롭고 통찰력 있는 이야기를 들을 수 있다.     연세대 노어노문학과 남혜현 교수는 “지금까지 양국 간 많은 행사가 개최됐지만 대부분 정부 주도 행사였으며 젊은 세대의 관심을 끌기에는 역부족이었다.  ”며 ”이번 행사는 러시아를 전공하는 학생들이 주도적으로 기획해 젊은 학생들의 눈높이에서 러시아를 소개하는 행사로, 진정한 의미에서 양국 간 이해의 폭을 넓히는 소중한 걸음이 될 것“이라고 행사의 의미를 전했다.   안드레이 쿨릭(Andrey Kulik) 주한러시아대사는 축사를 통해 “대한민국의 명문 사학인 연세대학교에서 ‘러시아 문화의 달’을 진행한 것은 양국 관계의 발전에 새로운 자극이 되고, 이를 통해 양국민의 상호 이해는 더욱 굳건해질 것”이라며 연세대에 감사 인사를 전했다.  '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[17689][\"content_ori\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07f58823-92e2-4bbf-86e9-10b6aabfbed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content = \"\"\"경상북도경제진흥원(원장 전창록)이 경북도 내 중소기업들에게 코로나19 비대면 마케팅을 적극 지원, 7개월 만에 1천700억원의 판매 성과를 냈다.\n",
    "경북경제진흥원(이하 진흥원)은 올들어 코로나19 확산으로 기존 오프라인 대면사업을 비대면 온라인 사업으로 발 빠르게 전환해 중소기업에게 ▷온라인 화상 수출상담회 ▷온라인 유통채널 확대 ▷온라인 판매 기획전(경북세일페스타) 등 비대면 사업을 집중 지원하고 있다.\n",
    "국내는 지난 5월부터 쿠팡·위메프·티몬·공영홈쇼핑·우체국 등 대형 쇼핑몰에 입점, 온라인 기획전 '경북세일페스타' 등으로 11월말 기준 1천700여억원의 판매 실적을 올렸다.\n",
    "또 해외는 아마존·이베이·티몰·쇼피·큐텐 등 유명 온라인몰 입점을 통해 크고 작은 성과를 내고 있다.\n",
    "이와 함께 중소기업 150곳이 참여하는 온라인 화상 수출상담회를 통해서도 11억7천만달러의 수출 상담 성과를 거뒀다.\n",
    "전창록 경북경제진흥원장은 \"내년에도 경북도 내 중소기업의 온라인 마케팅 성장 단계별 맞춤형 지원과 온라인 수출상담회 확대 등을 확대할 계획\"이라고 밝혔다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a13c95cd-fb65-44b1-b0ef-a78a7abaa2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15043    [세계로 가는 우리 농식품 공기업 시리즈 ③ 농축산식품] 모바일 상담회, 온라인 유...\n",
       "15039    [세계로 가는 우리 농식품 공기업 시리즈 ③ 농축산식품]  비대면·건강·가정식이 키...\n",
       "17226                코로나19 속 中企 수출 날개 달아준 ‘2020 지페어코리아’ 폐막\n",
       "4045                  \"차를 타보지도 않고 산다고?\"…코로나로 온라인 차 판매에 관심 \n",
       "4148                  “4월이면 월급 줄돈 바닥난다” 요즘 中企 유행어 ‘부도 확진자’\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_to_recom(test_content,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8bb55448-afbd-42fa-88df-5546aeb449c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'신종 코로나바이러스 감염증(코로나19)으로 수출이 부진한 상황에서도 K-푸드의 수출 실적은 지난해 대비 5.  6%가 증가했다.   농림축산식품부에 따르면 올해 8월까지 농림축산식품 수출액은 지난해 동기 대비 4.  7% 증가한 48억4567만 달러(한화 약 5조7500억원)를 기록했다.   코로나19로 인한 세계적인 경기 침체, 국가 이동제한 등 어려운 상황에서도 ▶건강식품에 대한 수요를 공략한 김치 ▶장기보관이 가능한 라면 ▶집밥에 활용되는 소스류 등 품목별 인기 ▶비대면·온라인 마케팅을 적극적으로 추진한 전략이 주효했다는 분석이다.   농식품부와 한국농수산식품유통공사(이하 aT)는 올해 코로나19로 인한 대내외 수출환경 변화에 발맞춰 신규시장 진출 전략을 온라인 기반으로 전면 개편했다.   국제식품박람회가 취소되거나 연기됨에 따라 이를 대체하기 위해 모바일 기반의 상담회를 기획·추진해 성공을 거뒀다.   aT는 지난 4월 중화권을 시작으로, 싱가포르·일본·러시아·카자흐스탄·브라질·미국 등 10개국 유력 식품바이어와 K-푸드 온라인·모바일 수출상담회를 개최하고 있다.   그 결과 10개국 158개 바이어와 200여 개 국내 수출업체가 온라인에서 만나 3000만 달러(한화 약 356억원) 규모의 상담 실적을 거뒀다.   특히 aT는 해외지사를 활용해 현지의 유력 바이어를 직접 섭외하고, 상담 전에 온라인 웹 상세 페이지 제작, 상담회 참여 바이어 대상 제품 소개, 샘플 배송 등으로 상담회를 적극적으로 지원했다.   또한 농식품부와 aT는 중국 주요 전자상거래 플랫폼과 중국 시장 진출을 희망하는 한국 기업의 연결에도 적극적으로 나서고 있다.   경쟁력 있는 K-푸드 제품을 소개하고, 플랫폼 내 한국 상품을 전용으로 판매하는 ‘한국관’을 개설하는 등 온라인 유통채널 발굴에 집중하고 있다.   지난 2월 중국 인터넷 전자상거래 쇼핑몰인 ‘징동닷컴’ 한국관에서는 코로나19로 인해 현지 물류·배송이 여의치 않은 상황에 신속하게 대응해 ‘홈코노미 품목’인 스낵류·장류·액상차 판촉을 펼쳐 15만3000달러(약 1억8000만원)의 매출을 올렸다.   7월에도 중국 알리바바그룹의 허마센셩 온라인몰에 국가관으로는 최초로 한국관을 개설해 신선버섯·장류 등 약 20여 개의 신규 제품을 입점시켰다.   지난달 24일에는 한·중 수교 28주년 기념일에 맞춰 핀둬둬 한국관을 개설하고 100여 개 한국 식품으로 ‘한국의 날’ 생방송을 진행해 최대 접속자 수 53만 명을 기록하기도 했다.  '"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[15043][\"content_ori\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a1064890-d6c9-43dd-9b62-84b37bb79c25",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class TfidfVectorizer(CountVectorizer):\n",
      "    r\"\"\"Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "\n",
      "    Equivalent to :class:`CountVectorizer` followed by\n",
      "    :class:`TfidfTransformer`.\n",
      "\n",
      "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    input : {'filename', 'file', 'content'}, default='content'\n",
      "        - If `'filename'`, the sequence passed as an argument to fit is\n",
      "          expected to be a list of filenames that need reading to fetch\n",
      "          the raw content to analyze.\n",
      "\n",
      "        - If `'file'`, the sequence items must have a 'read' method (file-like\n",
      "          object) that is called to fetch the bytes in memory.\n",
      "\n",
      "        - If `'content'`, the input is expected to be a sequence of items that\n",
      "          can be of type string or byte.\n",
      "\n",
      "    encoding : str, default='utf-8'\n",
      "        If bytes or files are given to analyze, this encoding is used to\n",
      "        decode.\n",
      "\n",
      "    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      "        Instruction on what to do if a byte sequence is given to analyze that\n",
      "        contains characters not of the given `encoding`. By default, it is\n",
      "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "        values are 'ignore' and 'replace'.\n",
      "\n",
      "    strip_accents : {'ascii', 'unicode'}, default=None\n",
      "        Remove accents and perform other character normalization\n",
      "        during the preprocessing step.\n",
      "        'ascii' is a fast method that only works on characters that have\n",
      "        an direct ASCII mapping.\n",
      "        'unicode' is a slightly slower method that works on any characters.\n",
      "        None (default) does nothing.\n",
      "\n",
      "        Both 'ascii' and 'unicode' use NFKD normalization from\n",
      "        :func:`unicodedata.normalize`.\n",
      "\n",
      "    lowercase : bool, default=True\n",
      "        Convert all characters to lowercase before tokenizing.\n",
      "\n",
      "    preprocessor : callable, default=None\n",
      "        Override the preprocessing (string transformation) stage while\n",
      "        preserving the tokenizing and n-grams generation steps.\n",
      "        Only applies if ``analyzer`` is not callable.\n",
      "\n",
      "    tokenizer : callable, default=None\n",
      "        Override the string tokenization step while preserving the\n",
      "        preprocessing and n-grams generation steps.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      "        Whether the feature should be made of word or character n-grams.\n",
      "        Option 'char_wb' creates character n-grams only from text inside\n",
      "        word boundaries; n-grams at the edges of words are padded with space.\n",
      "\n",
      "        If a callable is passed it is used to extract the sequence of features\n",
      "        out of the raw, unprocessed input.\n",
      "\n",
      "        .. versionchanged:: 0.21\n",
      "            Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n",
      "            is first read from the file and then passed to the given callable\n",
      "            analyzer.\n",
      "\n",
      "    stop_words : {'english'}, list, default=None\n",
      "        If a string, it is passed to _check_stop_list and the appropriate stop\n",
      "        list is returned. 'english' is currently the only supported string\n",
      "        value.\n",
      "        There are several known issues with 'english' and you should\n",
      "        consider an alternative (see :ref:`stop_words`).\n",
      "\n",
      "        If a list, that list is assumed to contain stop words, all of which\n",
      "        will be removed from the resulting tokens.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "        If None, no stop words will be used. max_df can be set to a value\n",
      "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
      "        words based on intra corpus document frequency of terms.\n",
      "\n",
      "    token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
      "        Regular expression denoting what constitutes a \"token\", only used\n",
      "        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      "        or more alphanumeric characters (punctuation is completely ignored\n",
      "        and always treated as a token separator).\n",
      "\n",
      "        If there is a capturing group in token_pattern then the\n",
      "        captured group content, not the entire match, becomes the token.\n",
      "        At most one capturing group is permitted.\n",
      "\n",
      "    ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      "        The lower and upper boundary of the range of n-values for different\n",
      "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      "        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
      "        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
      "        only bigrams.\n",
      "        Only applies if ``analyzer`` is not callable.\n",
      "\n",
      "    max_df : float or int, default=1.0\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly higher than the given threshold (corpus-specific\n",
      "        stop words).\n",
      "        If float in range [0.0, 1.0], the parameter represents a proportion of\n",
      "        documents, integer absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    min_df : float or int, default=1\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly lower than the given threshold. This value is also\n",
      "        called cut-off in the literature.\n",
      "        If float in range of [0.0, 1.0], the parameter represents a proportion\n",
      "        of documents, integer absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    max_features : int, default=None\n",
      "        If not None, build a vocabulary that only consider the top\n",
      "        max_features ordered by term frequency across the corpus.\n",
      "\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    vocabulary : Mapping or iterable, default=None\n",
      "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      "        indices in the feature matrix, or an iterable over terms. If not\n",
      "        given, a vocabulary is determined from the input documents.\n",
      "\n",
      "    binary : bool, default=False\n",
      "        If True, all non-zero term counts are set to 1. This does not mean\n",
      "        outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      "        is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
      "\n",
      "    dtype : dtype, default=float64\n",
      "        Type of the matrix returned by fit_transform() or transform().\n",
      "\n",
      "    norm : {'l1', 'l2'}, default='l2'\n",
      "        Each output row will have unit norm, either:\n",
      "\n",
      "        - 'l2': Sum of squares of vector elements is 1. The cosine\n",
      "          similarity between two vectors is their dot product when l2 norm has\n",
      "          been applied.\n",
      "        - 'l1': Sum of absolute values of vector elements is 1.\n",
      "          See :func:`preprocessing.normalize`.\n",
      "\n",
      "    use_idf : bool, default=True\n",
      "        Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n",
      "\n",
      "    smooth_idf : bool, default=True\n",
      "        Smooth idf weights by adding one to document frequencies, as if an\n",
      "        extra document was seen containing every term in the collection\n",
      "        exactly once. Prevents zero divisions.\n",
      "\n",
      "    sublinear_tf : bool, default=False\n",
      "        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    vocabulary_ : dict\n",
      "        A mapping of terms to feature indices.\n",
      "\n",
      "    fixed_vocabulary_ : bool\n",
      "        True if a fixed vocabulary of term to indices mapping\n",
      "        is provided by the user.\n",
      "\n",
      "    idf_ : array of shape (n_features,)\n",
      "        The inverse document frequency (IDF) vector; only defined\n",
      "        if ``use_idf`` is True.\n",
      "\n",
      "    stop_words_ : set\n",
      "        Terms that were ignored because they either:\n",
      "\n",
      "          - occurred in too many documents (`max_df`)\n",
      "          - occurred in too few documents (`min_df`)\n",
      "          - were cut off by feature selection (`max_features`).\n",
      "\n",
      "        This is only available if no vocabulary was given.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
      "\n",
      "    TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
      "        matrix of counts.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The ``stop_words_`` attribute can get large and increase the model size\n",
      "    when pickling. This attribute is provided only for introspection and can\n",
      "    be safely removed using delattr or set to None before pickling.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "    >>> corpus = [\n",
      "    ...     'This is the first document.',\n",
      "    ...     'This document is the second document.',\n",
      "    ...     'And this is the third one.',\n",
      "    ...     'Is this the first document?',\n",
      "    ... ]\n",
      "    >>> vectorizer = TfidfVectorizer()\n",
      "    >>> X = vectorizer.fit_transform(corpus)\n",
      "    >>> vectorizer.get_feature_names_out()\n",
      "    array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
      "           'this'], ...)\n",
      "    >>> print(X.shape)\n",
      "    (4, 9)\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        *,\n",
      "        input=\"content\",\n",
      "        encoding=\"utf-8\",\n",
      "        decode_error=\"strict\",\n",
      "        strip_accents=None,\n",
      "        lowercase=True,\n",
      "        preprocessor=None,\n",
      "        tokenizer=None,\n",
      "        analyzer=\"word\",\n",
      "        stop_words=None,\n",
      "        token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
      "        ngram_range=(1, 1),\n",
      "        max_df=1.0,\n",
      "        min_df=1,\n",
      "        max_features=None,\n",
      "        vocabulary=None,\n",
      "        binary=False,\n",
      "        dtype=np.float64,\n",
      "        norm=\"l2\",\n",
      "        use_idf=True,\n",
      "        smooth_idf=True,\n",
      "        sublinear_tf=False,\n",
      "    ):\n",
      "\n",
      "        super().__init__(\n",
      "            input=input,\n",
      "            encoding=encoding,\n",
      "            decode_error=decode_error,\n",
      "            strip_accents=strip_accents,\n",
      "            lowercase=lowercase,\n",
      "            preprocessor=preprocessor,\n",
      "            tokenizer=tokenizer,\n",
      "            analyzer=analyzer,\n",
      "            stop_words=stop_words,\n",
      "            token_pattern=token_pattern,\n",
      "            ngram_range=ngram_range,\n",
      "            max_df=max_df,\n",
      "            min_df=min_df,\n",
      "            max_features=max_features,\n",
      "            vocabulary=vocabulary,\n",
      "            binary=binary,\n",
      "            dtype=dtype,\n",
      "        )\n",
      "\n",
      "        self._tfidf = TfidfTransformer(\n",
      "            norm=norm, use_idf=use_idf, smooth_idf=smooth_idf, sublinear_tf=sublinear_tf\n",
      "        )\n",
      "\n",
      "    # Broadcast the TF-IDF parameters to the underlying transformer instance\n",
      "    # for easy grid search and repr\n",
      "\n",
      "    @property\n",
      "    def norm(self):\n",
      "        \"\"\"Norm of each row output, can be either \"l1\" or \"l2\".\"\"\"\n",
      "        return self._tfidf.norm\n",
      "\n",
      "    @norm.setter\n",
      "    def norm(self, value):\n",
      "        self._tfidf.norm = value\n",
      "\n",
      "    @property\n",
      "    def use_idf(self):\n",
      "        \"\"\"Whether or not IDF re-weighting is used.\"\"\"\n",
      "        return self._tfidf.use_idf\n",
      "\n",
      "    @use_idf.setter\n",
      "    def use_idf(self, value):\n",
      "        self._tfidf.use_idf = value\n",
      "\n",
      "    @property\n",
      "    def smooth_idf(self):\n",
      "        \"\"\"Whether or not IDF weights are smoothed.\"\"\"\n",
      "        return self._tfidf.smooth_idf\n",
      "\n",
      "    @smooth_idf.setter\n",
      "    def smooth_idf(self, value):\n",
      "        self._tfidf.smooth_idf = value\n",
      "\n",
      "    @property\n",
      "    def sublinear_tf(self):\n",
      "        \"\"\"Whether or not sublinear TF scaling is applied.\"\"\"\n",
      "        return self._tfidf.sublinear_tf\n",
      "\n",
      "    @sublinear_tf.setter\n",
      "    def sublinear_tf(self, value):\n",
      "        self._tfidf.sublinear_tf = value\n",
      "\n",
      "    @property\n",
      "    def idf_(self):\n",
      "        \"\"\"Inverse document frequency vector, only defined if `use_idf=True`.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        ndarray of shape (n_features,)\n",
      "        \"\"\"\n",
      "        return self._tfidf.idf_\n",
      "\n",
      "    @idf_.setter\n",
      "    def idf_(self, value):\n",
      "        self._validate_vocabulary()\n",
      "        if hasattr(self, \"vocabulary_\"):\n",
      "            if len(self.vocabulary_) != len(value):\n",
      "                raise ValueError(\n",
      "                    \"idf length = %d must be equal to vocabulary size = %d\"\n",
      "                    % (len(value), len(self.vocabulary))\n",
      "                )\n",
      "        self._tfidf.idf_ = value\n",
      "\n",
      "    def _check_params(self):\n",
      "        if self.dtype not in FLOAT_DTYPES:\n",
      "            warnings.warn(\n",
      "                \"Only {} 'dtype' should be used. {} 'dtype' will \"\n",
      "                \"be converted to np.float64.\".format(FLOAT_DTYPES, self.dtype),\n",
      "                UserWarning,\n",
      "            )\n",
      "\n",
      "    def fit(self, raw_documents, y=None):\n",
      "        \"\"\"Learn vocabulary and idf from training set.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        raw_documents : iterable\n",
      "            An iterable which generates either str, unicode or file objects.\n",
      "\n",
      "        y : None\n",
      "            This parameter is not needed to compute tfidf.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        self : object\n",
      "            Fitted vectorizer.\n",
      "        \"\"\"\n",
      "        self._check_params()\n",
      "        self._warn_for_unused_params()\n",
      "        X = super().fit_transform(raw_documents)\n",
      "        self._tfidf.fit(X)\n",
      "        return self\n",
      "\n",
      "    def fit_transform(self, raw_documents, y=None):\n",
      "        \"\"\"Learn vocabulary and idf, return document-term matrix.\n",
      "\n",
      "        This is equivalent to fit followed by transform, but more efficiently\n",
      "        implemented.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        raw_documents : iterable\n",
      "            An iterable which generates either str, unicode or file objects.\n",
      "\n",
      "        y : None\n",
      "            This parameter is ignored.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X : sparse matrix of (n_samples, n_features)\n",
      "            Tf-idf-weighted document-term matrix.\n",
      "        \"\"\"\n",
      "        self._check_params()\n",
      "        X = super().fit_transform(raw_documents)\n",
      "        self._tfidf.fit(X)\n",
      "        # X is already a transformed view of raw_documents so\n",
      "        # we set copy to False\n",
      "        return self._tfidf.transform(X, copy=False)\n",
      "\n",
      "    def transform(self, raw_documents):\n",
      "        \"\"\"Transform documents to document-term matrix.\n",
      "\n",
      "        Uses the vocabulary and document frequencies (df) learned by fit (or\n",
      "        fit_transform).\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        raw_documents : iterable\n",
      "            An iterable which generates either str, unicode or file objects.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X : sparse matrix of (n_samples, n_features)\n",
      "            Tf-idf-weighted document-term matrix.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self, msg=\"The TF-IDF vectorizer is not fitted\")\n",
      "\n",
      "        X = super().transform(raw_documents)\n",
      "        return self._tfidf.transform(X, copy=False)\n",
      "\n",
      "    def _more_tags(self):\n",
      "        return {\"X_types\": [\"string\"], \"_skip_test\": True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    " \n",
    "print(inspect.getsource(TfidfVectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1edc193f-612e-4d67-b8d5-aba1dcf13412",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6620\\2884782658.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mTfidfVectorizer_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     def __init__(\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dfdcaf-c14d-4e51-9655-174fc6549337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
