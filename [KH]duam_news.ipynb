{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609ec095-1682-47a0-bb16-f5384a6df249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n190 page 기준 8분 정도\\n휴일 때 대략 100~200 page\\n평일 때 대략 400~500 page \\n\\n한 페이지에 15 기사\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "190 page 기준 8분 정도\n",
    "휴일 때 대략 100~200 page\n",
    "평일 때 대략 400~500 page \n",
    "\n",
    "한 페이지에 15 기사\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d568c3-4f98-474c-9b0e-1bc7cd99e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import pandas as pd\n",
    "from tqdm import notebook\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "start_date = date(2022, 1, 1)\n",
    "end_date = date(2022, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb8e7ed-7439-4b79-9178-e7ccd1005cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(url):\n",
    "\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.content, 'html.parser', from_encoding='cp949')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62167232-5ae6-4483-a6fb-c81ed629653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(year,month,day):\n",
    "\n",
    "    \n",
    "    date_data = pd.DataFrame()\n",
    "    \n",
    "    year = str(year)\n",
    "    month = str(month).zfill(2)\n",
    "    day = str(day).zfill(2)\n",
    "    date = year+month+day\n",
    "    \n",
    "    \n",
    "    # 마지막 페이지 번호 추출\n",
    "    url = f\"https://news.daum.net/breakingnews/society?page=999&regDate={date}\"\n",
    "    soup = create_soup(url)\n",
    "    last_page = soup.find(\"em\",attrs ={\"class\": \"num_page\"})\n",
    "    last_page_num = int(re.sub(r'[^0-9]', '', last_page.text))\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in notebook.tqdm(range(1,last_page_num+1)):\n",
    "        url = f\"https://news.daum.net/breakingnews/society?page={i}&regDate={date}\"\n",
    "        soup = create_soup(url)\n",
    "\n",
    "        \n",
    "    \n",
    "        # 뉴스 리스트가 포함된 속성 추출\n",
    "        news_list_part = soup.find('ul', attrs={'class':'list_news2 list_allnews'})\n",
    "        \n",
    "        # COLUME : press\n",
    "        press_list = []\n",
    "        press = news_list_part.find_all('span', attrs={'class':'info_news'})\n",
    "        for _span in press:\n",
    "            press_list.append(_span.text.split()[0])\n",
    "        \n",
    "        # 뉴스 링크 리스트 추출\n",
    "        # COLUME : link\n",
    "        link_list = []\n",
    "        news_link_list = news_list_part.find_all('a',attrs={\"class\":\"link_thumb\"})\n",
    "        for news_link in news_link_list:\n",
    "            link_list.append(news_link[\"href\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # COLUME : articleid\n",
    "        articleid = []\n",
    "        for link in link_list:\n",
    "            articleid.append(link.split(\"/\")[-1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 뉴스 링크 리스트 추출\n",
    "        # COLUME : link\n",
    "        news_link_list = news_list_part.find_all('a')\n",
    "\n",
    "        # 링크마다 요청\n",
    "        # COLUME : title, content ,journalist\n",
    "        title_list = []\n",
    "        journalist_list = []\n",
    "        content_list = []\n",
    "\n",
    "        for link in link_list:\n",
    "            url = link\n",
    "            article_soup = create_soup(url)\n",
    "            article_soup = article_soup.find(\"article\")\n",
    "            \n",
    "            ### title\n",
    "            title = article_soup.find(\"h3\", attrs={'class':\"tit_view\"})\n",
    "            title_list.append(title.text)\n",
    "            \n",
    "            ### journalist\n",
    "            journalist = article_soup.find(\"span\", attrs={'class':\"txt_info\"})\n",
    "            journalist_list.append(journalist.text)\n",
    "            \n",
    "            ### content\n",
    "            content = article_soup.find(\"div\", attrs={'class':\"article_view\"})\n",
    "            \n",
    "            if content.select_one(\"p.link_figure\") != None:\n",
    "                content.select_one(\"p.link_figure\").decompose()\n",
    "                \n",
    "            \n",
    "            news_contents = content.find_all(\"p\")\n",
    "            news_content= \"\"\n",
    "            for _p in news_contents:\n",
    "                news_content += _p.text\n",
    "            news_content.replace(\"\\xa0\",\"\")\n",
    "            content_list.append(news_content)\n",
    "            \n",
    "                \n",
    "                \n",
    "        data =  pd.DataFrame(zip(articleid, title_list,journalist_list,press_list,content_list, link_list))\n",
    "            \n",
    "        date_data = pd.concat([date_data, data])\n",
    "    \n",
    "    return date_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906257eb-f15a-4333-960b-e0b32e594589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bac49695-bb2b-4e90-9af0-c22284e5f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dateutil.rrule import rrule, DAILY\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# for date in notebook.tqdm(list(rrule(DAILY, dtstart=start_date, until=end_date))):\n",
    "#     df = pd.concat([df, get_articles(date.year, date.month, date.day)])\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df = get_articles(date.year, date.month, date.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17729119-d9ee-46ff-8ac6-d162bf20cec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ee365e9d7b4a05ade2203d2662971b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = get_articles(2022, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fdb8394-f64e-4a0b-a876-4ea1ee381a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHTTPSConnectionPool(host='v.daum.net', port=443): Max retries exceeded with url: /v/20220101121016727 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000020C9A0B23A0>: Failed to establish a new connection: [WinError 10060] 연결된 구성원으로부터 응답이 없어 연결하지 못했거나, 호스트로부터 응답이 없어 연결이 끊어졌습니다'))\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "HTTPSConnectionPool(host='v.daum.net', port=443): Max retries exceeded with url: /v/20220101121016727 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000020C9A0B23A0>: Failed to establish a new connection: [WinError 10060] 연결된 구성원으로부터 응답이 없어 연결하지 못했거나, 호스트로부터 응답이 없어 연결이 끊어졌습니다'))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6152dbed-b65f-4e01-bd28-4cff7df55296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[앵커] 검찰과 공수처의 '대장동 개발 특혜'와 '고발 사주' 의혹 수사가 결국, 해를 넘겼습니다. 정치적 부담을 덜기 위해선 대선 전까지 결론을 내야 하는 만큼 수사기관의 부담이 더욱 커졌습니다. 김다연 기자가 보도합니다. [기자] 지난해 9월 검찰과 공수처는 각각 '대장동 개발 특혜'와 '고발 사주' 의혹 수사에 착수했습니다. 하지만 주요 관계자 사망과 잇단 신병확보 실패 등의 변수들로 차질이 생기면서 두 수사 모두 해를 넘겼습니다. 대장동 수사팀은 유동규, 김만배, 남욱, 정영학, 정민용 등 핵심 5인방을 재판에 넘긴 뒤 개발사업 보고 계통에 이름을 올렸던 성남시 관계자를 차례로 불러 조사하는 등 '윗선' 수사에 집중하는 듯했습니다. 하지만 초과이익 환수 조항 삭제에 관여한 것으로 알려진 유한기 전 개발사업본부장과 김문기 개발1처장이 숨지면서 수사에 제동이 걸렸습니다. 어수선한 분위기 속에 당시 성남시장이던 이재명 더불어민주당 대선 후보는 물론 최측근인 정진상 전 정책실장에 대해서도 조사 한 번 못해봤습니다. 화천대유 측으로부터 50억 원을 받거나 받기로 약속했다는 이른바 '50억 클럽' 등 로비 의혹 수사도 남은 과제입니다. 곽상도 전 의원에 대해선 구속영장이 한 차례 기각된 뒤 한 달째 신병처리 방향을 정하지 못하고 있고, 박영수 전 특검과 권순일 전 대법관 등 다른 인사와 관련해서도 아직 뚜렷한 혐의점이 발견되지 않은 것으로 알려졌습니다. 공수처의 '고발 사주' 수사도 매듭짓지 못한 채 공전하긴 마찬가지입니다. 고발장 작성과 전달에 관여한 혐의를 받는 손준성 검사와 김웅 국민의힘 의원 등 핵심인물이 불구속 기소되는 선에서 수사가 일단락될 거란 전망이 나왔지만, 손 검사 신병확보 시도가 잇따라 불발된 뒤 수사는 사실상 멈췄습니다. 당시 검찰총장으로 의혹의 최정점에 있는 윤석열 국민의힘 대선 후보에 대한 조사도 아직인데 김진욱 공수처장은 선거에 영향이 없도록 처리하겠다는 입장만 되풀이하고 있습니다. 유력 대선 후보 관련이라는 점에서 시작부터 관심이 집중됐던 대장동과 고발 사주 의혹 수사 모두 실체를 드러내지 못한 채 나란히 한 해를 마무리했습니다. 정치적 논란을 피하려면 다음 달 대선 후보 등록 전까지는 수사가 마무리돼야 한다는 지적이 나오는 가운데 검찰과 공수처가 선거 전 수사 돌파구를 찾을 수 있을지 주목됩니다. YTN 김다연입니다. YTN 김다연 (kimdy0818@ytn.co.kr)※ '당신의 제보가 뉴스가 됩니다' YTN은 여러분의 소중한 제보를 기다립니다. [카카오톡] YTN을 검색해 채널 추가 [전화] 02-398-8585 [메일] social@ytn.co.kr [온라인 제보] www.ytn.co.kr[저작권자(c) YTN & YTN plus 무단전재 및 재배포 금지]‘싱글몰트위스키 vs 스카치위스키‘ 다니엘이 설명해준다!  대한민국 24시간 뉴스채널 YTN [LIVE 보기]  이 시각 코로나19 확진자 및 예방접종 현황을 확인하세요.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[32][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7677184b-2ce7-49f7-acd1-62a383f274d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2742 entries, 0 to 13\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       2742 non-null   object\n",
      " 1   1       2742 non-null   object\n",
      " 2   2       2742 non-null   object\n",
      " 3   3       2742 non-null   object\n",
      " 4   4       2742 non-null   object\n",
      " 5   5       2742 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 150.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cc3eb-366f-4bfd-afde-29aecdad4dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
