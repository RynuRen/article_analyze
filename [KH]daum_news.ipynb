{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609ec095-1682-47a0-bb16-f5384a6df249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n190 page 기준 8분 정도\\n휴일 때 대략 100~200 page\\n평일 때 대략 400~500 page \\n\\n한 페이지에 15 기사\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "190 page 기준 8분 정도\n",
    "휴일 때 대략 100~200 page\n",
    "평일 때 대략 400~500 page \n",
    "\n",
    "한 페이지에 15 기사\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d568c3-4f98-474c-9b0e-1bc7cd99e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import pandas as pd\n",
    "from tqdm import notebook\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "start_date = date(2022, 1, 1)\n",
    "end_date = date(2022, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb8e7ed-7439-4b79-9178-e7ccd1005cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(url):\n",
    "\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.content, 'html.parser', from_encoding='cp949')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "62167232-5ae6-4483-a6fb-c81ed629653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(year,month,day):\n",
    "\n",
    "    \n",
    "    date_data = pd.DataFrame()\n",
    "    \n",
    "    year = str(year)\n",
    "    month = str(month).zfill(2)\n",
    "    day = str(day).zfill(2)\n",
    "    date = year+month+day\n",
    "    \n",
    "    \n",
    "    # 마지막 페이지 번호 추출\n",
    "    url = f\"https://news.daum.net/breakingnews/society?page=999&regDate={date}\"\n",
    "    soup = create_soup(url)\n",
    "    last_page = soup.find(\"em\",attrs ={\"class\": \"num_page\"})\n",
    "    last_page_num = int(re.sub(r'[^0-9]', '', last_page.text))\n",
    "\n",
    "    \n",
    "    # 본문 외 내용 제외를 위한 키워드 리스트\n",
    "    no_get_list = [\"[카카오톡]\", \"[전화]\", \"[메일]\",\"[온라인 제보]\", \"재배포 금지\", \"※ '당신의 제보가 뉴스가 됩니다'\", \"무단전재\", \"@\"]\n",
    "    \n",
    "    replace_list = [\"\\xa0\", \"\\u2008\"]\n",
    "    \n",
    "    for i in notebook.tqdm(range(1,last_page_num+1)):\n",
    "        url = f\"https://news.daum.net/breakingnews/society?page={i}&regDate={date}\"\n",
    "        soup = create_soup(url)\n",
    "\n",
    "        \n",
    "    \n",
    "        # 뉴스 리스트가 포함된 속성 추출\n",
    "        news_list_part = soup.find('ul', attrs={'class':'list_news2 list_allnews'})\n",
    "        \n",
    "        # COLUME : press\n",
    "        press_list = []\n",
    "        press = news_list_part.find_all('span', attrs={'class':'info_news'})\n",
    "        for _span in press:\n",
    "            press_list.append(_span.text.split()[0])\n",
    "        \n",
    "        # 뉴스 링크 리스트 추출\n",
    "        # COLUME : link\n",
    "        link_list = []\n",
    "        news_link_list = news_list_part.find_all('a',attrs={\"class\":\"link_thumb\"})\n",
    "        for news_link in news_link_list:\n",
    "            link_list.append(news_link[\"href\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # COLUME : articleid\n",
    "        articleid = []\n",
    "        for link in link_list:\n",
    "            articleid.append(link.split(\"/\")[-1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 뉴스 링크 리스트 추출\n",
    "        # COLUME : link\n",
    "        news_link_list = news_list_part.find_all('a')\n",
    "\n",
    "        # 링크마다 요청\n",
    "        # COLUME : title, content ,journalist\n",
    "        title_list = []\n",
    "        journalist_list = []\n",
    "        content_list = []\n",
    "\n",
    "        for idx, link in enumerate(link_list):\n",
    "            url = link\n",
    "            article_soup = create_soup(url)\n",
    "            article_soup = article_soup.find(\"article\")\n",
    "            \n",
    "            ### title\n",
    "            title = article_soup.find(\"h3\", attrs={'class':\"tit_view\"})\n",
    "            title_list.append(title.text)\n",
    "            \n",
    "            ### journalist\n",
    "            journalist = article_soup.find(\"span\", attrs={'class':\"txt_info\"})\n",
    "            journalist_list.append(journalist.text)\n",
    "            \n",
    "            ### content\n",
    "            content = article_soup.find(\"div\", attrs={'class':\"article_view\"})\n",
    "            \n",
    "            # 이미지 제거 \n",
    "            if content.select_one(\"p.link_figure\") != None:\n",
    "                content.select_one(\"p.link_figure\").decompose()\n",
    "                \n",
    "            \n",
    "            news_contents = content.find_all(\"p\")\n",
    "            news_content= \"\"\n",
    "            \n",
    "            # p text 확인 후 \n",
    "            for _p in news_contents:\n",
    "                skip_p = False\n",
    "                \n",
    "                tmp = _p.text\n",
    "                \n",
    "                # 주로 \\과 관련된 문자를 \" \"로 변경\n",
    "                for replace_word in replace_list:\n",
    "                    tmp = tmp.replace(replace_word,\" \")\n",
    "                    \n",
    "                # tmp = tmp.replace(journalist_list[idx],\"\")\n",
    "                \n",
    "                # 예외가 포함된 문장을 건너뛰기\n",
    "                for no in no_get_list :\n",
    "                    if no in tmp :\n",
    "                        skip_p = True\n",
    "                        break;\n",
    "                        \n",
    "                if skip_p :\n",
    "                    continue\n",
    "\n",
    "                        \n",
    "                news_content += tmp\n",
    "\n",
    "                \n",
    "            content_list.append(news_content)\n",
    "            \n",
    "                \n",
    "                \n",
    "        data =  pd.DataFrame(zip(articleid, title_list,journalist_list,press_list,content_list, link_list))\n",
    "            \n",
    "        date_data = pd.concat([date_data, data])\n",
    "    \n",
    "    return date_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "906257eb-f15a-4333-960b-e0b32e594589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [카카오톡] YTN을 검색해 채널 추가 [전화] 02-398-8585 [메일] social@ytn.co.kr [온라인 제보] www.ytn.co.kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bac49695-bb2b-4e90-9af0-c22284e5f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dateutil.rrule import rrule, DAILY\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# for date in notebook.tqdm(list(rrule(DAILY, dtstart=start_date, until=end_date))):\n",
    "#     df = pd.concat([df, get_articles(date.year, date.month, date.day)])\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df = get_articles(date.year, date.month, date.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "17729119-d9ee-46ff-8ac6-d162bf20cec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ce4bffc4e44f6a90177b7fa3c3aefb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = get_articles(2022, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8fdb8394-f64e-4a0b-a876-4ea1ee381a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHTTPSConnectionPool(host='v.daum.net', port=443): Max retries exceeded with url: /v/20220101121016727 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000020C9A0B23A0>: Failed to establish a new connection: [WinError 10060] 연결된 구성원으로부터 응답이 없어 연결하지 못했거나, 호스트로부터 응답이 없어 연결이 끊어졌습니다'))\\n\""
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "HTTPSConnectionPool(host='v.daum.net', port=443): Max retries exceeded with url: /v/20220101121016727 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000020C9A0B23A0>: Failed to establish a new connection: [WinError 10060] 연결된 구성원으로부터 응답이 없어 연결하지 못했거나, 호스트로부터 응답이 없어 연결이 끊어졌습니다'))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7677184b-2ce7-49f7-acd1-62a383f274d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 141 entries, 0 to 12\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       141 non-null    object\n",
      " 1   1       141 non-null    object\n",
      " 2   2       141 non-null    object\n",
      " 3   3       141 non-null    object\n",
      " 4   4       141 non-null    object\n",
      " 5   5       141 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 7.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cc3eb-366f-4bfd-afde-29aecdad4dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff238e-b664-4e10-82d5-bb6fb661c12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
